{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL3_4.ipynb.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewJYEfNU5W1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SzQJblb5X60",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetSigmoid(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetSigmoid, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.sigmoid(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.sigmoid(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjrHPcqo54AM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetRelu(nn.Module):\n",
        "    def __init__(self, d=0):\n",
        "        super(NetRelu, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.drop = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(F.relu(self.conv1(x)), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.dropout(F.relu(self.conv2(x)), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.dropout(F.relu(self.fc1(x)), p=self.drop)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs3byN4DYAAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetReluNorm(nn.Module):\n",
        "    def __init__(self, d=0):\n",
        "        super(NetReluNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        self.drop = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.bn1(F.relu(self.conv1(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.dropout(self.bn2(F.relu(self.conv2(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.dropout(F.relu(self.fc1(x)), p=self.drop)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXY3gOYrIRr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetReluXavier(nn.Module):\n",
        "    def __init__(self, d=0):\n",
        "        super(NetReluXavier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        nn.init.xavier_normal_(self.conv1.weight)\n",
        "        self.bn1 = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        nn.init.xavier_normal_(self.conv2.weight)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        nn.init.xavier_normal_(self.fc1.weight)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        nn.init.xavier_normal_(self.fc2.weight)\n",
        "        self.drop = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.bn1(F.relu(self.conv1(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.dropout(self.bn2(F.relu(self.conv2(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.dropout(F.relu(self.fc1(x)), p=self.drop)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBEmFslXJ85L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NetReluKaiming(nn.Module):\n",
        "    def __init__(self, d=0):\n",
        "        super(NetReluKaiming, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight)\n",
        "        self.bn1 = nn.BatchNorm2d(20)\n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        nn.init.kaiming_normal_(self.conv2.weight)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.fc1 = nn.Linear(4*4*50, 500)\n",
        "        nn.init.kaiming_normal_(self.fc1.weight)\n",
        "        self.fc2 = nn.Linear(500, 10)\n",
        "        nn.init.kaiming_normal_(self.fc2.weight)\n",
        "        self.drop = d\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.dropout(self.bn1(F.relu(self.conv1(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.dropout(self.bn2(F.relu(self.conv2(x))), p=self.drop)\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 4*4*50)\n",
        "        x = F.dropout(F.relu(self.fc1(x)), p=self.drop)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvSBvKbe5e1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7siCMjQu5hJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIeDaCsa5lHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(flag, drop=0):\n",
        "    # Training settings\n",
        "    batchSize = 64\n",
        "    testBatchSize = 1000\n",
        "    epochs= 10\n",
        "    learningRate = 0.01\n",
        "    momentum = 0.5\n",
        "    use_cuda = False\n",
        "    seed = 1\n",
        "    saveModel = False\n",
        "    logInterval = 1000\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=batchSize, shuffle=True, **kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize((0.1307,), (0.3081,))\n",
        "                       ])),\n",
        "        batch_size=batchSize, shuffle=True, **kwargs)\n",
        "\n",
        "\n",
        "    if flag == \"sig\":\n",
        "        model = NetSigmoid().to(device)\n",
        "    elif flag == \"relu\":\n",
        "        model = NetRelu(drop).to(device)\n",
        "    elif flag == \"normRelu\":\n",
        "        model = NetReluNorm(drop).to(device)\n",
        "    elif flag == \"NetReluXavier\":\n",
        "       model =  NetReluXavier().to(device)\n",
        "    elif flag == \"NetReluKaiming\":\n",
        "        model =  NetReluKaiming().to(device)\n",
        "        \n",
        "    optimizer = optim.SGD(model.parameters(), lr=learningRate, momentum=momentum)\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(logInterval, model, device, train_loader, optimizer, epoch)\n",
        "        test(model, device, test_loader)\n",
        "        \n",
        "    print(\"Train Accuracy\")\n",
        "    test(model, device, train_loader)\n",
        "    print(\"Test Accuracy\", end=\"\")\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "#     if (args.save_model):\n",
        "#         torch.save(model.state_dict(),\"mnist_cnn.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erFUxmLorUfa",
        "colab_type": "text"
      },
      "source": [
        "<h4>a) We observe that relu performs better sigmoid on both testing and training. Sigmoid provides healthy gradients only in a very small range as the value of function saturates towards extremes. There is no such issue with Relu thereforce we observe a better performance.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD-VsTAy1D1U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "outputId": "cb71768e-af80-45a4-e9ee-b8385f3836e0"
      },
      "source": [
        "# if __name__ == '__main__':\n",
        "main(\"sig\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.342638\n",
            "\n",
            "Test set: Average loss: 2.3035, Accuracy: 1287/10000 (13%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.299908\n",
            "\n",
            "Test set: Average loss: 2.2758, Accuracy: 1491/10000 (15%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.288799\n",
            "\n",
            "Test set: Average loss: 1.8694, Accuracy: 4889/10000 (49%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.816753\n",
            "\n",
            "Test set: Average loss: 0.8458, Accuracy: 7832/10000 (78%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.889295\n",
            "\n",
            "Test set: Average loss: 0.5315, Accuracy: 8470/10000 (85%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.622900\n",
            "\n",
            "Test set: Average loss: 0.4108, Accuracy: 8797/10000 (88%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.544359\n",
            "\n",
            "Test set: Average loss: 0.3411, Accuracy: 9007/10000 (90%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.382309\n",
            "\n",
            "Test set: Average loss: 0.2958, Accuracy: 9123/10000 (91%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.267932\n",
            "\n",
            "Test set: Average loss: 0.2570, Accuracy: 9219/10000 (92%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.411476\n",
            "\n",
            "Test set: Average loss: 0.2308, Accuracy: 9317/10000 (93%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.2448, Accuracy: 55753/60000 (93%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.2308, Accuracy: 9317/10000 (93%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiG0NSgB7vdD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "8d1a41fb-d9b5-4f22-80ad-2684113dbb96"
      },
      "source": [
        "main(\"relu\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300039\n",
            "\n",
            "Test set: Average loss: 0.1017, Accuracy: 9661/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.145728\n",
            "\n",
            "Test set: Average loss: 0.0611, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052906\n",
            "\n",
            "Test set: Average loss: 0.0563, Accuracy: 9807/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.020191\n",
            "\n",
            "Test set: Average loss: 0.0405, Accuracy: 9864/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010667\n",
            "\n",
            "Test set: Average loss: 0.0387, Accuracy: 9870/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.126077\n",
            "\n",
            "Test set: Average loss: 0.0337, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.030172\n",
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 9873/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.004710\n",
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 9874/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.082814\n",
            "\n",
            "Test set: Average loss: 0.0295, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.119259\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0181, Accuracy: 59677/60000 (99%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0320, Accuracy: 9892/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPi_7F1auHkF",
        "colab_type": "text"
      },
      "source": [
        "<h4>b) It was observed that a dropout of 0.25 gives optimal performance. The accuracy is 98.97%.<br>\n",
        "Following are accuracy of various settings given as a tuple of (training, testing) accuracy:<br>\n",
        "0.25: (59458/60000, 9897/10000)<br>\n",
        "0.5: (59174/60000, 9860/10000)<br>\n",
        "0.75: (57948/60000, 9693/10000)<br>\n",
        "1: (5923/60000, 980/10000)<br><br>\n",
        "\n",
        "The performance with 100% dropout is justified as whole network is wasted and network don't learn at all. It's peroformance should be equivalent to random guessing(0.1). It is observed that accuracy decreases monotonically as we increase dropout from 0.25 to 1.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQ_rawatO2eL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "428d8f6f-f64b-4e3a-caba-457474d9e52b"
      },
      "source": [
        "dropout = [0.25, 0.5, 0.75, 0.99]\n",
        "for d in dropout:\n",
        "    print(\"Dropout =\",d)\n",
        "    main(\"relu\", d)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dropout = 0.25\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.293511\n",
            "\n",
            "Test set: Average loss: 0.1045, Accuracy: 9657/10000 (97%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.113460\n",
            "\n",
            "Test set: Average loss: 0.0743, Accuracy: 9751/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.049316\n",
            "\n",
            "Test set: Average loss: 0.0543, Accuracy: 9826/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.046828\n",
            "\n",
            "Test set: Average loss: 0.0480, Accuracy: 9841/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.130320\n",
            "\n",
            "Test set: Average loss: 0.0532, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.015400\n",
            "\n",
            "Test set: Average loss: 0.0394, Accuracy: 9869/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.057019\n",
            "\n",
            "Test set: Average loss: 0.0371, Accuracy: 9888/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.033667\n",
            "\n",
            "Test set: Average loss: 0.0358, Accuracy: 9880/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.051206\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9890/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.022096\n",
            "\n",
            "Test set: Average loss: 0.0354, Accuracy: 9887/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0287, Accuracy: 59458/60000 (99%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0331, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Dropout = 0.5\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.395315\n",
            "\n",
            "Test set: Average loss: 0.1451, Accuracy: 9534/10000 (95%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.210271\n",
            "\n",
            "Test set: Average loss: 0.1030, Accuracy: 9689/10000 (97%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.121088\n",
            "\n",
            "Test set: Average loss: 0.0764, Accuracy: 9760/10000 (98%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029761\n",
            "\n",
            "Test set: Average loss: 0.0668, Accuracy: 9784/10000 (98%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.127274\n",
            "\n",
            "Test set: Average loss: 0.0707, Accuracy: 9782/10000 (98%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.031683\n",
            "\n",
            "Test set: Average loss: 0.0543, Accuracy: 9822/10000 (98%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.084478\n",
            "\n",
            "Test set: Average loss: 0.0568, Accuracy: 9817/10000 (98%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.028680\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 9838/10000 (98%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.061065\n",
            "\n",
            "Test set: Average loss: 0.0485, Accuracy: 9837/10000 (98%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.012035\n",
            "\n",
            "Test set: Average loss: 0.0458, Accuracy: 9863/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0453, Accuracy: 59174/60000 (99%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0424, Accuracy: 9860/10000 (99%)\n",
            "\n",
            "Dropout = 0.75\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.291414\n",
            "\n",
            "Test set: Average loss: 0.2852, Accuracy: 9078/10000 (91%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.228279\n",
            "\n",
            "Test set: Average loss: 0.1931, Accuracy: 9406/10000 (94%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.261405\n",
            "\n",
            "Test set: Average loss: 0.1586, Accuracy: 9487/10000 (95%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.127592\n",
            "\n",
            "Test set: Average loss: 0.1389, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.353065\n",
            "\n",
            "Test set: Average loss: 0.1318, Accuracy: 9579/10000 (96%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.050561\n",
            "\n",
            "Test set: Average loss: 0.1174, Accuracy: 9634/10000 (96%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.117507\n",
            "\n",
            "Test set: Average loss: 0.1146, Accuracy: 9635/10000 (96%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.046443\n",
            "\n",
            "Test set: Average loss: 0.1001, Accuracy: 9666/10000 (97%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.056580\n",
            "\n",
            "Test set: Average loss: 0.0997, Accuracy: 9685/10000 (97%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.127918\n",
            "\n",
            "Test set: Average loss: 0.1015, Accuracy: 9685/10000 (97%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.1123, Accuracy: 57948/60000 (97%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.1006, Accuracy: 9693/10000 (97%)\n",
            "\n",
            "Dropout = 0.99\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 242.318588\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: nan\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: nan, Accuracy: 5923/60000 (10%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: nan, Accuracy: 980/10000 (10%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS6BixJjwxmT",
        "colab_type": "text"
      },
      "source": [
        "<h4>c) Relu+BatchNorm+Dropout of 0.25: 9919/10000<br>\n",
        "Relu+BatchNorm: 9892/10000<br><br>\n",
        "We observe that batch normalisation with dropout performs slightly better then model without dropout setting. Both batch normalisation and dropout to used to perform regularisation. When we use both of them together they complement each other and we get better results.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWwsrN1m9kzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "1bd3eceb-ba73-4cb0-c301-9893370a488d"
      },
      "source": [
        "main(\"normRelu\", 0.25)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.393392\n",
            "\n",
            "Test set: Average loss: 0.0636, Accuracy: 9823/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.031083\n",
            "\n",
            "Test set: Average loss: 0.0522, Accuracy: 9829/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.024056\n",
            "\n",
            "Test set: Average loss: 0.0379, Accuracy: 9870/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006460\n",
            "\n",
            "Test set: Average loss: 0.0373, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.153912\n",
            "\n",
            "Test set: Average loss: 0.0385, Accuracy: 9875/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004283\n",
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.029979\n",
            "\n",
            "Test set: Average loss: 0.0299, Accuracy: 9901/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.039421\n",
            "\n",
            "Test set: Average loss: 0.0274, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.012920\n",
            "\n",
            "Test set: Average loss: 0.0234, Accuracy: 9926/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.004943\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0157, Accuracy: 59731/60000 (100%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0268, Accuracy: 9919/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXRgdtsyhzlo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cfeb0574-7973-4450-ed43-1beedf7dffee"
      },
      "source": [
        "main(\"normRelu\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 16384/9912422 [00:00<01:12, 136204.94it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3759110.86it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 442426.65it/s]\n",
            "  1%|          | 16384/1648877 [00:00<00:11, 147024.67it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 2161377.45it/s]                            \n",
            "8192it [00:00, 184644.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.350755\n",
            "\n",
            "Test set: Average loss: 0.0505, Accuracy: 9852/10000 (99%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.074585\n",
            "\n",
            "Test set: Average loss: 0.0393, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.019437\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9889/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.008904\n",
            "\n",
            "Test set: Average loss: 0.0319, Accuracy: 9883/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010379\n",
            "\n",
            "Test set: Average loss: 0.0280, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.026324\n",
            "\n",
            "Test set: Average loss: 0.0276, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.010272\n",
            "\n",
            "Test set: Average loss: 0.0273, Accuracy: 9905/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001519\n",
            "\n",
            "Test set: Average loss: 0.0327, Accuracy: 9894/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.037827\n",
            "\n",
            "Test set: Average loss: 0.0296, Accuracy: 9904/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.012564\n",
            "\n",
            "Test set: Average loss: 0.0312, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0028, Accuracy: 59990/60000 (100%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0312, Accuracy: 9892/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPC6pkzz8-BD",
        "colab_type": "text"
      },
      "source": [
        "<h4>d) (training accuracy, testing accuracy)<br>\n",
        "Xavier+Relu+BatchNorm: (59993/60000, 9914/10000)<br>\n",
        "Kaiming+Relu+BatchNorm: (59989/60000, 9896/10000)<br><br>\n",
        "It is  observed that Xavier performs slightly better than Kaiming. This is probably because Xavier gives a better generalisation performance.</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpZKLMQc9-0x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "4612e2ed-ef8f-4bc0-cf58-19e3c185373c"
      },
      "source": [
        "main(\"NetReluXavier\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.390522\n",
            "\n",
            "Test set: Average loss: 0.0507, Accuracy: 9833/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.035497\n",
            "\n",
            "Test set: Average loss: 0.0394, Accuracy: 9876/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.042008\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 9892/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.019695\n",
            "\n",
            "Test set: Average loss: 0.0353, Accuracy: 9878/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.015287\n",
            "\n",
            "Test set: Average loss: 0.0311, Accuracy: 9898/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004349\n",
            "\n",
            "Test set: Average loss: 0.0307, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.004149\n",
            "\n",
            "Test set: Average loss: 0.0284, Accuracy: 9909/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001774\n",
            "\n",
            "Test set: Average loss: 0.0286, Accuracy: 9907/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.003407\n",
            "\n",
            "Test set: Average loss: 0.0276, Accuracy: 9911/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000871\n",
            "\n",
            "Test set: Average loss: 0.0278, Accuracy: 9914/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0025, Accuracy: 59993/60000 (100%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0278, Accuracy: 9914/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8awNCw0-53C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "2eb00cd2-051f-46e4-ea79-bacd1081d49a"
      },
      "source": [
        "main(\"NetReluKaiming\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 4.032737\n",
            "\n",
            "Test set: Average loss: 0.0629, Accuracy: 9799/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.041501\n",
            "\n",
            "Test set: Average loss: 0.0491, Accuracy: 9835/10000 (98%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.062049\n",
            "\n",
            "Test set: Average loss: 0.0402, Accuracy: 9868/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.009703\n",
            "\n",
            "Test set: Average loss: 0.0363, Accuracy: 9875/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007942\n",
            "\n",
            "Test set: Average loss: 0.0339, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.003158\n",
            "\n",
            "Test set: Average loss: 0.0335, Accuracy: 9893/10000 (99%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.006966\n",
            "\n",
            "Test set: Average loss: 0.0320, Accuracy: 9897/10000 (99%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.002109\n",
            "\n",
            "Test set: Average loss: 0.0321, Accuracy: 9884/10000 (99%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.007412\n",
            "\n",
            "Test set: Average loss: 0.0309, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.000824\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 9896/10000 (99%)\n",
            "\n",
            "Train Accuracy\n",
            "\n",
            "Test set: Average loss: 0.0045, Accuracy: 59989/60000 (100%)\n",
            "\n",
            "Test Accuracy\n",
            "Test set: Average loss: 0.0313, Accuracy: 9896/10000 (99%)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj6_Uv5oLQ2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}